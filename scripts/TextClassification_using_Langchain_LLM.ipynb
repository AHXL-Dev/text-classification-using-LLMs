{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Modelling search classification script\n",
    "* Script inspired and adapted from categorize_expenses_with_validation.ipynb, Thu Vu, https://github.com/thu-vu92/local-llms-analyse-finance/\n",
    "* The dataset is from the Kaggle , details are as follows:  Anshul Chaudhary, and Muskan Risinghani. (2023). Airline Reviews [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/4044107\n",
    "\n",
    "* This model is trained on the BA_AirlineReviews.csv dataset.\n",
    "* The model goes through each row with the prompt and the question (which is what we want to theme by) and then classifies each row with a Yes or No. We also put the full LLMs explanation per row in a seperate column\n",
    "* At the moment we are just passing 100 rows from the dataset.\n",
    "* Please DO NOT run the script without fully understanding the code. In Particular knowing the capabilities of your CPU and adjusting the number of workers and the number of rows being passed to the model accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying with LangChain - multiprocessing\n",
    "\n",
    "import time\n",
    "from langchain.schema import HumanMessage\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"BA_AirlineReviews.csv\")\n",
    "df = df.sample(n=100)\n",
    "df = df.rename(columns={\"Unnamed: 0\":\"RowID\"})\n",
    "def init_chat_ollama(model_name):\n",
    "    \"\"\"\n",
    "    Initialize a ChatOllama object.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the local Llama model.\n",
    "\n",
    "    Returns:\n",
    "        ChatOllama: An instance of the ChatOllama model.\n",
    "    \"\"\"\n",
    "    return ChatOllama(model=model_name,temperature=0.2)\n",
    "def classify_row_with_ollama(row, theme, chat_model):\n",
    "    \"\"\"\n",
    "    Classifies a single row based on a theme.\n",
    "\n",
    "    Args:\n",
    "        row from the DataFrame.\n",
    "        theme (str): The theme to analyze for the row.\n",
    "        chat_model (what we passed from the init_chat_ollama)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (RowID, classification result)\n",
    "    \"\"\"\n",
    "    #pick up the ReviewBody row and prepare the prompt to be passed\n",
    "    text = row[\"ReviewBody\"]\n",
    "    prompt = f\"Does the following text mention issues explicitly related to the '{theme}'? Respond with 'Yes' or 'No'. Text: '{text}'\"\n",
    "    \n",
    "    # we now use the chat model and pass the prompt as a Human message. If there is an unclassified row, pass it as Cant Classify\n",
    "    try:\n",
    "        response = chat_model([HumanMessage(content=prompt)])\n",
    "        response_text = response.content.strip()  \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row['RowID']}: {e}\")\n",
    "        response_text = \"Cant classify\"\n",
    "    \n",
    "    return row['RowID'], response_text\n",
    "\n",
    "def thread_classify_allrows(df, theme, chat_model):\n",
    "    \"\"\"\n",
    "    Uses the classify_row_with_ollama to classify all rows using multi-threading\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with a \"TextData\" column.\n",
    "        theme (str): The theme to analyze for each row.\n",
    "        chat_model: the chat model being passed\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column named after the theme.\n",
    "    \"\"\"\n",
    "\n",
    "    # two lists initiated theme_tags to get the 'yes', 'no' value. theme_tags_explain is for the explanation given by the model.\n",
    "    theme_tags = []\n",
    "    theme_tags_explain = []\n",
    "    #For my PC using 10 workers, need to adjust based on your max cores\n",
    "    #Returns key with async tasks and values are the RowIDs\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        async_tasks = {executor.submit(classify_row_with_ollama, row, theme, chat_model): row[\"RowID\"] for _, row in df.iterrows()}\n",
    "        \n",
    "        #Looping through the async_task to get the tuple results \n",
    "        for result in async_tasks:\n",
    "            row_id = async_tasks[result]\n",
    "            try:\n",
    "                # This should now return (RowID, response_text)\n",
    "                row_result = result.result()  \n",
    "                theme_tags_explain.append(row_result[1])\n",
    "                #print(f\"Row {row_result[0]}: Response: {row_result[1]}\")  # Optional, for real-time feedback, might remove this\n",
    "                if row_result[1].lower().startswith(\"yes\"):\n",
    "                    theme_tags.append(\"yes\")\n",
    "                elif row_result[1].lower().startswith(\"no\"):\n",
    "                    theme_tags.append(\"no\")\n",
    "                else:\n",
    "                    theme_tags.append(\"Cant classify\")\n",
    "            except Exception as e:\n",
    "                print(f\"Cant classify row {row_id}: {e}\")\n",
    "                theme_tags.append(\"Cant classify\")\n",
    "    \n",
    "    explain_column_name = f\"{theme}_EXPLAIN\" #Name the column name\n",
    "    # Dynamically name the column based on the theme\n",
    "    df[theme] = theme_tags\n",
    "    df[explain_column_name] = theme_tags_explain\n",
    "    return df\n",
    "  \n",
    "# Timing the entire process\n",
    "start_time = time.time()  # Record the start time\n",
    "\n",
    "# Example using Llama 3.18b\n",
    "model_name = \"llama3.1:8b\"\n",
    "theme_to_search = \"cleanliness-related issues, specifically dirty or unclean conditions\"\n",
    "\n",
    "# Initialize the ChatOllama session\n",
    "chat_ollama = init_chat_ollama(model_name)\n",
    "\n",
    "# Classify rows in the DataFrame using multiprocessing\n",
    "tagged_df_second_model = thread_classify_allrows(df, theme_to_search, chat_ollama)\n",
    "\n",
    "# Save the results to a CSV\n",
    "tagged_df_second_model.to_csv(\"cleanliness_test_second_model_with_multiprocessing.csv\")\n",
    "\n",
    "end_time = time.time()  # Record the end time\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-testing-B5K3D4iW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
